{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3-ADM-new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRrFUkOluBPz"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qg3FsZOqg3y",
        "outputId": "16cc2c5f-a6e0-4fd6-bffe-2785ce83007d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVLSBzX01fQR"
      },
      "source": [
        "\n",
        "**1.1** **Get the list of animes**\n",
        "\n",
        "With the for loop I get the url of all the pages (50 per page except the last one which has less,383 pag).\n",
        "We 'send' a request to the page, to collect the ursl.\n",
        "We read the html and take the href attributes of the tag.\n",
        "The command tqdm is useful because it shows the progress status.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8axghAGyg81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b00ba3-2aa8-46d5-9111-ef1cf515fb4b"
      },
      "source": [
        "urls=[]\n",
        "for i in tqdm(range(0,20000,50)):\n",
        "  url='https://myanimelist.net/topanime.php?limit='+str(i)\n",
        "  \n",
        "  soup= BeautifulSoup(requests.get(url).text,'html.parser')\n",
        "\n",
        "  for tag in soup.find_all('tr'):\n",
        "    links=tag.find_all('a')\n",
        "    for l in links:\n",
        "      if type(l.get('id'))== str and len(l.contents[0]) >1:\n",
        "        urls.append(l.get('href'))\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400/400 [02:47<00:00,  2.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHalfFSds4WE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqfeOZHI-zS5"
      },
      "source": [
        "len(urls)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbV4Ob8sd7ZE"
      },
      "source": [
        "We need to save it as text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGwH3_oKd5oQ"
      },
      "source": [
        "with open('urls.txt', 'w', encoding='utf-8') as f: #open the file con write, chiude da solo in automatico, lo trovo in file sample data\n",
        "    for line in urls:\n",
        "        f.write(line)\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8bTjaGoAcAB"
      },
      "source": [
        "#!ls pages/3pag & just a check, this is used to see what there is inside 3pag\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9032XW3mrUu"
      },
      "source": [
        "**1.2** **Crawl animes**\n",
        "\n",
        "Create the folders. Put all of them under \"pages\". Each folder has a name that refers to the number of the page from which the links it contains come from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX8zgYdls6X2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPgUUBWkp_A5",
        "outputId": "37cc831a-9d2a-487a-a5e0-496586e5f05b"
      },
      "source": [
        "import os\n",
        "for page in tqdm(range(1, 384)):\n",
        "    folder = \"page\"+str(page)\n",
        "    path = \"/content/drive/MyDrive/Anime_folder\"+folder\n",
        "    os.mkdir(path)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 383/383 [00:01<00:00, 328.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiSpQapy9_pV"
      },
      "source": [
        "!rm -rf pages\n",
        "   #to delete folders that are not needed."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqMJOqYI6e9"
      },
      "source": [
        "Try for the first 150:ok.\n",
        "Try to collect them in group of 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qpBSbL1rWDK",
        "outputId": "b7534794-cca8-4abd-9e51-6d8daebf6c2e"
      },
      "source": [
        "for page in tqdm(range(0, 3)):  # page 1 --> 383\n",
        "    folder = \"/content/drive/MyDrive/Anime_folderpage\"+str(page+1)\n",
        "    update_page = 50*page\n",
        "    for i in range(0,50):   # 1 -> 50\n",
        "        url = f'{urls[update_page+i]}'\n",
        "        response = requests.get(url)   \n",
        "        filename = r\"\"+folder+\"/anime_\"+str(update_page+i+1)+\".html\"\n",
        "        with open(filename,'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [01:01<00:00, 20.50s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ZvxWZ1s8As",
        "outputId": "2d5cd633-2734-439a-b638-1d0534adbf22"
      },
      "source": [
        "for page in tqdm(range(4, 8)):  # page 1 --> 383\n",
        "    folder = \"/content/drive/MyDrive/Anime_folderpage\"+str(page+1)\n",
        "    update_page = 50*page\n",
        "    for i in range(0,50):   # 1 -> 50\n",
        "        url = f'{urls[update_page+i]}'\n",
        "        response = requests.get(url)   \n",
        "        filename = r\"\"+folder+\"/anime_\"+str(update_page+i+1)+\".html\"\n",
        "        with open(filename,'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:31<00:00, 22.99s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apBfP7ILvR_Q"
      },
      "source": [
        "**1.3**\n",
        "Parsing downloaded pages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iUE5U1rvatp"
      },
      "source": [
        "'''col = ['animeTitle', 'animeType', 'animeNumEpisode', 'releaseDate', 'endDate', 'animeNumMembers', 'animeScore', \\\n",
        "           'animeUsers', 'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', \\\n",
        "           'animeVoices', 'animeStaff']\n",
        "           '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA2PVk-JvuDl"
      },
      "source": [
        "def AnimeTitle(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    animeTitle=soup.find(\"h1\", attrs = {\"class\": \"title-name h1_bold_none\"}).string\n",
        "    return(animeTitle)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "yKjQLeCx0RBP",
        "outputId": "7a37a908-e69e-4259-f3d7-0661bbf81172"
      },
      "source": [
        "AnimeTitle('/content/drive/MyDrive/Anime_folderpage1/anime_10.html')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Gintama: The Final'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IteQnYb01tJa",
        "outputId": "fa133def-8db5-4db6-a05b-0382015ba791"
      },
      "source": [
        "animeScore('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.97"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz9pHA4Y0iLe"
      },
      "source": [
        "def animeScore(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    for i in range(10):\n",
        "      score=soup.find_all('div' ,attrs = {\"class\": \"score-label score-\"+str(i)})\n",
        "      for j in score:\n",
        "        return float(j.text)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDp237d2swz"
      },
      "source": [
        "def AnimeRank(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    rank=int(str(soup.find(class_=\"numbers ranked\").text).split('#')[-1])  \n",
        "    return(rank)\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS8aTJM937Mo",
        "outputId": "5983562b-0ade-49a0-86b0-383896dabeed"
      },
      "source": [
        "AnimeRank('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3_0oM3XGce_"
      },
      "source": [
        "import re"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY7AwoW46Sae"
      },
      "source": [
        "def animeNumberofepisode(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=soup.find(text=re.compile('Episodes:')).parent.parent.text.split()[-1]      \n",
        "    print(A)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2_pyaV6WtV",
        "outputId": "08686f34-abb8-4de7-d6dc-20304f126b4c"
      },
      "source": [
        "animeNumberofepisode('/content/drive/MyDrive/Anime_folderpage1/anime_1.html')\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV2PE96F6obI"
      },
      "source": [
        "def AnimePopularity(html):\n",
        "  with open(html,'r') as f:\n",
        "    soup= BeautifulSoup(f)\n",
        "    rank=int(str(soup.find(class_=\"numbers popularity\").text).split('#')[-1])  \n",
        "    return(rank)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf1KMAQS7Qfk",
        "outputId": "c15625e0-da2a-445c-d9fe-f37788eed765"
      },
      "source": [
        "AnimePopularity('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1cY2FcYBLth"
      },
      "source": [
        "def animeDescription(html_string): #I think that this function works well.\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    A=soup.find(itemprop=\"description\")\n",
        "    return A.get_text()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "Hj0pPZF1AmYu",
        "outputId": "577cbd51-2b97-4c0c-859d-87f08bc6f4a1"
      },
      "source": [
        "animeDescription('/content/drive/MyDrive/Anime_folderpage1/anime_10.html')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'New Gintama movie.'"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFTyrWTrB2hc"
      },
      "source": [
        "def animeUsers(html_string): #I think that this function works well.\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    A=soup.find(itemprop=\"ratingCount\")\n",
        "    return int(A.get_text())"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rraD5MNDRS9",
        "outputId": "473f716f-ce0a-4325-f28f-050253454c3a"
      },
      "source": [
        "animeUsers('/content/drive/MyDrive/Anime_folderpage1/anime_13.html')"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1208990"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ewri_C3HVYs"
      },
      "source": [
        "def animeType(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=soup.find(text=re.compile('Type:')).parent.parent.text.split()[-1]      \n",
        "    print(A)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hhVmC9jHmcX",
        "outputId": "54349e08-e287-4276-cc03-3ab52e7c44f1"
      },
      "source": [
        "animeType('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cGDulgiKnuQ"
      },
      "source": [
        " turn a list of strings in datetime format.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdOyFHydWkAn"
      },
      "source": [
        "def fai_date(a: list):\n",
        "    \n",
        "    string = ''\n",
        "    for i in a:\n",
        "        string += i\n",
        "        if i != a[-1]:\n",
        "            string += ' '\n",
        "    string = string.replace(',', '')\n",
        "    date = datetime.strptime(string, '%b %d %Y')\n",
        "    return date"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEe3_T98IVNa"
      },
      "source": [
        "def animeRelDate(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= fai_date(soup.find(text=re.compile('Aired:'), class_=\"dark_text\").parent.text.split()[1:4]  )   \n",
        "    print(A)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7rR3adhKGFa",
        "outputId": "d00faae5-82cd-4bdf-e4e7-dbba904e4936"
      },
      "source": [
        "animeRelDate('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2017-01-09 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U67bFhutLVF6"
      },
      "source": [
        "def animeEndDate(html_string):\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= fai_date(soup.find(text=re.compile('Aired:'), class_=\"dark_text\").parent.text.split()[5:8]   )  \n",
        "    print(A)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp3CHDn2Lo3g",
        "outputId": "697d443a-a26a-4260-9bf0-35c0473f5716"
      },
      "source": [
        "animeEndDate('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mar', '27,', '2017']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heZWMXDlMnUK"
      },
      "source": [
        "def animeRelated(html_string):  \n",
        "  animeRelated = []\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    related = soup.find_all(\"table\", {\"class\":\"anime_detail_related_anime\"})\n",
        "    for i in related:\n",
        "      links = i.find_all('a')\n",
        "      for link in links:        \n",
        "          animeRelated.append(f'{link.contents[0]}')\n",
        "\n",
        "  return animeRelated\n",
        "     "
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq3bCgAiNFDh",
        "outputId": "854afcbd-0b44-4f49-a51c-4c8ca1857dc9"
      },
      "source": [
        " animeRelated('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Gintama', 'Gintama°', 'Gintama.: Porori-hen']"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpAWBHSNwsh"
      },
      "source": [
        "def animeCharacter(html_string):\n",
        "  personaggi=[]\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= soup.find_all(class_=\"h3_characters_voice_actors\")    \n",
        "    for a in A:\n",
        "      personaggi.append(a.text)\n",
        "    return(personaggi) "
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW5v207NN83-",
        "outputId": "e0ea66df-751e-4632-840d-4aa14f46731a"
      },
      "source": [
        "animeCharacter('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sakata, Gintoki',\n",
              " 'Kagura',\n",
              " 'Katsura, Kotarou',\n",
              " 'Takasugi, Shinsuke',\n",
              " 'Shimura, Shinpachi',\n",
              " 'Kamui',\n",
              " 'Elizabeth',\n",
              " 'Imai, Nobume',\n",
              " 'Sadaharu',\n",
              " 'Sakamoto, Tatsuma']"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFddOxpnRMpS"
      },
      "source": [
        "def animeVoices(html_string):\n",
        "  personaggi=[]\n",
        "  with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A= soup.find_all(class_=\"va-t ar pl4 pr4\")   \n",
        "    for a in A:\n",
        "      personaggi.append(a.text)\n",
        "\n",
        "      \n",
        "    return(personaggi) \n",
        "   "
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE79rQnJRjMM",
        "outputId": "452d3eb3-7d03-49ae-b1f9-c6be1b8d1634"
      },
      "source": [
        "animeVoices('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nSugita, Tomokazu\\nJapanese\\n',\n",
              " '\\nKugimiya, Rie\\nJapanese\\n',\n",
              " '\\nIshida, Akira\\nJapanese\\n',\n",
              " '\\nKoyasu, Takehito\\nJapanese\\n',\n",
              " '\\nSakaguchi, Daisuke\\nJapanese\\n',\n",
              " '\\nHino, Satoshi\\nJapanese\\n',\n",
              " '\\nHirano, Aya\\nJapanese\\n',\n",
              " '\\nTakahashi, Mikako\\nJapanese\\n',\n",
              " '\\nMiki, Shinichiro\\nJapanese\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y2D2DWcT0b5"
      },
      "source": [
        "def createstaff(ll):\n",
        "    \"\"\"\n",
        "    puts all relevant text in a list after cleaning it up a little\n",
        "    \"\"\"\n",
        "    l = []\n",
        "    for i in ll:\n",
        "        if i['href'].startswith('https://myanimelist.net/people/') and i['href'] not in l:\n",
        "            if i.text != '\\n\\n':\n",
        "                j = str(i.text).replace('\\n', '')\n",
        "                j = re.sub(' +', ' ', j)\n",
        "            if j[0] == ' ':\n",
        "                j = j[1:]\n",
        "            if j[-1] == ' ':\n",
        "                j = j[:-1]\n",
        "            l.append(j)\n",
        "    return l      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def animeStaff(html_string):\n",
        "   with open(html_string, 'r') as f:\n",
        "    soup = BeautifulSoup(f, 'html.parser')\n",
        "    \n",
        "    A=  createstaff(soup.find_all('a', href=True))     \n",
        "    print(A)\n",
        "  \n",
        "  "
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx5QG6KgUHKj",
        "outputId": "2f8548dd-5cbb-4669-b7f0-0662074b3fa4"
      },
      "source": [
        "animeStaff('/content/drive/MyDrive/Anime_folderpage1/anime_11.html')"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sugita, Tomokazu', 'Sugita, Tomokazu', 'Kugimiya, Rie', 'Kugimiya, Rie', 'Ishida, Akira', 'Ishida, Akira', 'Koyasu, Takehito', 'Koyasu, Takehito', 'Sakaguchi, Daisuke', 'Sakaguchi, Daisuke', 'Hino, Satoshi', 'Hino, Satoshi', 'Hirano, Aya', 'Hirano, Aya', 'Takahashi, Mikako', 'Takahashi, Mikako', 'Miki, Shinichiro', 'Miki, Shinichiro', 'Miki, Shinichiro', 'Fujita, Youichi', 'Fujita, Youichi', 'Miyawaki, Chizuru', 'Miyawaki, Chizuru', 'Takamatsu, Shinji', 'Takamatsu, Shinji', 'Kobayashi, Katsuyoshi']\n"
          ]
        }
      ]
    }
  ]
}